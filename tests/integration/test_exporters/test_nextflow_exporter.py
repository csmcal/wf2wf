"""Tests for the Nextflow exporter functionality."""

from wf2wf.core import Workflow, Task, EnvironmentSpecificValue
from wf2wf.exporters.nextflow import from_workflow


class TestNextflowExporter:
    """Test the Nextflow exporter."""

    def test_export_simple_workflow(self, persistent_test_output):
        """Test exporting a simple linear workflow."""
        # Create a simple workflow
        wf = Workflow(name="simple_nextflow")

        task1 = Task(
            id="prepare_data",
            inputs=["input.txt"],
            outputs=["output.txt"],
        )
        task1.command.set_for_environment("python prepare.py input.txt output.txt", "shared_filesystem")
        task1.cpu.set_for_environment(2, "shared_filesystem")
        task1.mem_mb.set_for_environment(4096, "shared_filesystem")

        task2 = Task(
            id="analyze_data",
            inputs=["output.txt"],
            outputs=["results.txt"],
        )
        task2.command.set_for_environment("Rscript analyze.R output.txt results.txt", "shared_filesystem")
        task2.cpu.set_for_environment(4, "shared_filesystem")
        task2.mem_mb.set_for_environment(8192, "shared_filesystem")
        task2.container.set_for_environment("rocker/r-ver:4.2.0", "shared_filesystem")

        wf.add_task(task1)
        wf.add_task(task2)
        wf.add_edge("prepare_data", "analyze_data")

        # Export to Nextflow
        output_file = persistent_test_output / "simple_workflow.nf"
        from_workflow(wf, output_file, verbose=True, modular=False)

        # Check that file was created
        assert output_file.exists()

        # Read and verify content
        content = output_file.read_text()

        # Check header
        assert "#!/usr/bin/env nextflow" in content
        assert "simple_nextflow - Generated by wf2wf" in content
        assert "nextflow.enable.dsl=2" in content

        # Check process definitions (inline in non-modular mode)
        assert "process PREPARE_DATA {" in content
        assert "process ANALYZE_DATA {" in content

        # Check resource specifications
        assert "cpus 2" in content
        assert "memory '4.0.GB'" in content
        assert "cpus 4" in content
        assert "memory '8.0.GB'" in content

        # Check container
        assert "container 'rocker/r-ver:4.2.0'" in content

        # Check workflow definition
        assert "workflow {" in content

        # Check config file was created
        config_file = persistent_test_output / "nextflow.config"
        assert config_file.exists()

    def test_export_modular_workflow(self, persistent_test_output):
        """Test exporting workflow with separate module files."""
        wf = Workflow(name="modular_workflow")

        task1 = Task(
            id="process_one",
            outputs=["file1.txt"],
        )
        task1.command.set_for_environment("echo 'Process 1'", "shared_filesystem")
        task1.cpu.set_for_environment(1, "shared_filesystem")
        task1.mem_mb.set_for_environment(2048, "shared_filesystem")
        task1.container.set_for_environment("ubuntu:20.04", "shared_filesystem")

        task2 = Task(
            id="process_two",
            inputs=["file1.txt"],
            outputs=["file2.txt"],
        )
        task2.command.set_for_environment("echo 'Process 2'", "shared_filesystem")
        task2.cpu.set_for_environment(2, "shared_filesystem")
        task2.mem_mb.set_for_environment(4096, "shared_filesystem")

        wf.add_task(task1)
        wf.add_task(task2)
        wf.add_edge("process_one", "process_two")

        # Export with modular=True (default)
        output_file = persistent_test_output / "modular_main.nf"
        from_workflow(wf, output_file, verbose=True)

        # Check main file
        main_content = output_file.read_text()
        assert "include { PROCESS_ONE } from './modules/process_one'" in main_content
        assert "include { PROCESS_TWO } from './modules/process_two'" in main_content

        # Check module files were created
        modules_dir = persistent_test_output / "modules"
        assert modules_dir.exists()

        module1_file = modules_dir / "process_one.nf"
        module2_file = modules_dir / "process_two.nf"
        assert module1_file.exists()
        assert module2_file.exists()

        # Check module content
        module1_content = module1_file.read_text()
        assert "process PROCESS_ONE {" in module1_content
        assert "container 'ubuntu:20.04'" in module1_content
        assert "echo Process 1" in module1_content

        module2_content = module2_file.read_text()
        assert "process PROCESS_TWO {" in module2_content
        assert "cpus 2" in module2_content

    def test_export_with_config(self, persistent_test_output):
        """Test exporting workflow with configuration."""
        wf = Workflow(name="config_workflow")

        task = Task(
            id="analyze",
        )
        task.command.set_for_environment("python analyze.py --input ${params.input_data} --threads ${params.threads}", "shared_filesystem")
        task.cpu.set_for_environment(4, "shared_filesystem")
        task.mem_mb.set_for_environment(8192, "shared_filesystem")
        wf.add_task(task)

        # Export with separate config file
        output_file = persistent_test_output / "config_main.nf"
        config_file = persistent_test_output / "custom.config"

        from_workflow(wf, output_file, config_file=config_file)

        # Check config file content
        config_content = config_file.read_text()
        # Note: Since we removed the config parameter, this test may need adjustment
        # The config file should still be created with default values

    def test_export_with_containers_and_conda(self, persistent_test_output):
        """Test exporting workflow with container and conda specifications."""
        wf = Workflow(name="container_workflow")

        # Task with Docker container
        docker_task = Task(
            id="docker_task",
        )
        docker_task.command.set_for_environment("python script.py", "shared_filesystem")
        docker_task.container.set_for_environment("python:3.9-slim", "shared_filesystem")

        # Task with direct container reference
        container_task = Task(
            id="container_task",
        )
        container_task.command.set_for_environment("R script.R", "shared_filesystem")
        container_task.container.set_for_environment("bioconductor/release_core2", "shared_filesystem")

        # Task with conda environment
        conda_task = Task(
            id="conda_task",
        )
        conda_task.command.set_for_environment("snakemake --cores 1", "shared_filesystem")
        conda_task.conda.set_for_environment("envs/snakemake.yml", "shared_filesystem")

        wf.add_task(docker_task)
        wf.add_task(container_task)
        wf.add_task(conda_task)

        output_file = persistent_test_output / "container_workflow.nf"
        from_workflow(wf, output_file, modular=False)

        content = output_file.read_text()

        # Check container specifications (docker:// prefix should be removed)
        assert "container 'python:3.9-slim'" in content
        assert "container 'bioconductor/release_core2'" in content

    def test_export_with_comprehensive_resources(self, interactive_responses, persistent_test_output):
        """Test exporting workflow with comprehensive resource specifications."""
        # Set up specific responses for this test to ensure we get the expected resources
        interactive_responses.update({
            "CPU cores (default: 1): ": "16",
            "Memory (MB) (default: 4096): ": "32768",
            "Disk space (MB) (default: 4096): ": "102400",
            "GPU count (default: 0): ": "2",
            "Time limit (seconds) (default: 3600): ": "7200"
        })
        
        wf = Workflow(name="resource_workflow")

        task = Task(
            id="resource_intensive_task",
        )
        task.command.set_for_environment("python compute.py", "shared_filesystem")
        # Note: We're not setting resources here to trigger interactive prompts
        # The interactive_responses fixture will provide the values above

        wf.add_task(task)

        output_file = persistent_test_output / "resource_workflow.nf"
        from_workflow(wf, output_file, modular=False)

        content = output_file.read_text()

        # Check that all resource specifications are present
        assert "cpus 16" in content
        assert "memory '32 GB'" in content
        assert "disk '100 GB'" in content
        assert "accelerator 2" in content
        assert "time '2h'" in content

    def test_export_with_retry_and_error_handling(self, persistent_test_output):
        """Test exporting workflow with retry and error handling."""
        wf = Workflow(name="retry_workflow")

        task1 = Task(id="reliable_task")
        task1.command.set_for_environment("python reliable.py", "shared_filesystem")

        task2 = Task(id="flaky_task")
        task2.command.set_for_environment("python flaky.py", "shared_filesystem")
        # Explicitly set retries for the flaky task
        task2.set_retry_explicitly(3, "shared_filesystem")

        wf.add_task(task1)
        wf.add_task(task2)

        output_file = persistent_test_output / "retry_workflow.nf"
        from_workflow(wf, output_file, modular=False)

        content = output_file.read_text()

        # Check that only the flaky task has retry settings
        assert content.count("errorStrategy 'retry'") == 1
        assert "maxRetries 3" in content

        # Check that reliable task doesn't have retry settings
        reliable_section = content[
            content.find("process RELIABLE_TASK") : content.find("process FLAKY_TASK")
        ]
        assert "errorStrategy" not in reliable_section
        assert "maxRetries" not in reliable_section

    def test_export_with_scripts(self, persistent_test_output):
        """Test exporting workflow with script references."""
        wf = Workflow(name="script_workflow")

        task_with_script = Task(
            id="script_task",
            inputs=["data.txt"],
            outputs=["results.txt"],
        )
        task_with_script.script.set_for_environment("scripts/analysis.py", "shared_filesystem")
        task_with_script.command.set_for_environment("python scripts/analysis.py --input data.txt", "shared_filesystem")

        task_with_command = Task(
            id="command_task",
            outputs=["hello.txt"],
        )
        task_with_command.command.set_for_environment("echo 'Hello World' > hello.txt", "shared_filesystem")

        wf.add_task(task_with_script)
        wf.add_task(task_with_command)

        output_file = persistent_test_output / "script_workflow.nf"
        from_workflow(wf, output_file, modular=False)

        content = output_file.read_text()

        # Check script command
        assert "python scripts/analysis.py --input data.txt" in content

        # Check inline command
        assert "echo 'Hello World' > hello.txt" in content

    def test_export_complex_dependencies(self, persistent_test_output):
        """Test exporting workflow with complex dependency patterns."""
        wf = Workflow(name="complex_workflow")

        # Create a diamond dependency pattern
        task_a = Task(id="task_a", outputs=["a.txt"])
        task_a.command.set_for_environment("echo A", "shared_filesystem")
        task_b = Task(
            id="task_b", inputs=["a.txt"], outputs=["b.txt"]
        )
        task_b.command.set_for_environment("echo B", "shared_filesystem")
        task_c = Task(
            id="task_c", inputs=["a.txt"], outputs=["c.txt"]
        )
        task_c.command.set_for_environment("echo C", "shared_filesystem")
        task_d = Task(
            id="task_d", inputs=["b.txt", "c.txt"], outputs=["d.txt"]
        )
        task_d.command.set_for_environment("echo D", "shared_filesystem")

        wf.add_task(task_a)
        wf.add_task(task_b)
        wf.add_task(task_c)
        wf.add_task(task_d)

        wf.add_edge("task_a", "task_b")
        wf.add_edge("task_a", "task_c")
        wf.add_edge("task_b", "task_d")
        wf.add_edge("task_c", "task_d")

        output_file = persistent_test_output / "complex_workflow.nf"
        from_workflow(wf, output_file, modular=False)

        content = output_file.read_text()

        # Check that all processes are defined (non-modular mode defines processes inline)
        assert "process TASK_A {" in content
        assert "process TASK_B {" in content
        assert "process TASK_C {" in content
        assert "process TASK_D {" in content

        # Check that inputs and outputs are properly defined
        assert "input:" in content
        assert "output:" in content
        assert "a.txt" in content
        assert "b.txt" in content
        assert "c.txt" in content
        assert "d.txt" in content

    def test_name_sanitization(self, persistent_test_output):
        """Test that invalid process names are sanitized."""
        wf = Workflow(name="sanitize_workflow")

        # Tasks with problematic names
        task1 = Task(id="task-with-dashes")
        task1.command.set_for_environment("echo test1", "shared_filesystem")
        task2 = Task(id="task.with.dots")
        task2.command.set_for_environment("echo test2", "shared_filesystem")
        task3 = Task(id="123_numeric_start")
        task3.command.set_for_environment("echo test3", "shared_filesystem")
        task4 = Task(id="task with spaces")
        task4.command.set_for_environment("echo test4", "shared_filesystem")

        wf.add_task(task1)
        wf.add_task(task2)
        wf.add_task(task3)
        wf.add_task(task4)

        output_file = persistent_test_output / "sanitize_workflow.nf"
        from_workflow(wf, output_file, modular=False)

        content = output_file.read_text()

        # Check sanitized process names
        assert "process TASK_WITH_DASHES {" in content
        assert "process TASK_WITH_DOTS {" in content
        assert "process PROCESS_123_NUMERIC_START {" in content
        assert "process TASK_WITH_SPACES {" in content

    def test_memory_unit_conversion(self, persistent_test_output):
        """Test proper conversion of memory units."""
        wf = Workflow(name="memory_test")

        # Test various memory sizes
        task_mb = Task(id="task_mb")
        task_mb.cpu.set_for_environment(1, "shared_filesystem")
        task_mb.mem_mb.set_for_environment(512, "shared_filesystem")
        task_gb = Task(id="task_gb")
        task_gb.cpu.set_for_environment(1, "shared_filesystem")
        task_gb.mem_mb.set_for_environment(4096, "shared_filesystem")  # 4GB
        task_large = Task(id="task_large")
        task_large.cpu.set_for_environment(1, "shared_filesystem")
        task_large.mem_mb.set_for_environment(65536, "shared_filesystem")  # 64GB

        wf.add_task(task_mb)
        wf.add_task(task_gb)
        wf.add_task(task_large)

        output_file = persistent_test_output / "memory_test.nf"
        from_workflow(wf, output_file, modular=False)

        content = output_file.read_text()

        # Check memory conversions
        assert "cpus 1" in content
        assert "memory '512.MB'" in content  # Small memory stays in MB
        assert "cpus 1" in content
        assert "memory '4.0.GB'" in content  # >= 1GB converted to GB
        assert "cpus 1" in content
        assert "memory '64.0.GB'" in content  # Large memory in GB

    def test_time_unit_conversion(self, persistent_test_output):
        """Test proper conversion of time units."""
        wf = Workflow(name="time_test")

        # Test various time durations
        task_minutes = Task(id="task_min")
        task_minutes.cpu.set_for_environment(1, "shared_filesystem")
        task_minutes.time_s.set_for_environment(1800, "shared_filesystem")  # 30 minutes
        task_hours = Task(id="task_hour")
        task_hours.cpu.set_for_environment(1, "shared_filesystem")
        task_hours.time_s.set_for_environment(7200, "shared_filesystem")  # 2 hours
        task_long = Task(id="task_long")
        task_long.cpu.set_for_environment(1, "shared_filesystem")
        task_long.time_s.set_for_environment(28800, "shared_filesystem")  # 8 hours

        wf.add_task(task_minutes)
        wf.add_task(task_hours)
        wf.add_task(task_long)

        output_file = persistent_test_output / "time_test.nf"
        from_workflow(wf, output_file, modular=False)

        content = output_file.read_text()

        # Check time conversions
        assert "cpus 1" in content
        assert "time '30m'" in content  # < 1 hour in minutes
        assert "cpus 1" in content
        assert "time '2.0h'" in content  # >= 1 hour in hours
        assert "cpus 1" in content
        assert "time '8.0h'" in content  # Long duration in hours

    def test_complex_workflow_from_json(
        self, sample_workflow_json, persistent_test_output
    ):
        """Test exporting the sample JSON workflow to Nextflow."""
        from wf2wf.core import Workflow

        # Load the sample workflow
        wf = Workflow.load_json(sample_workflow_json)

        # Export to Nextflow
        output_file = persistent_test_output / "sample_workflow.nf"
        from_workflow(wf, output_file, verbose=True, modular=False)

        assert output_file.exists()

        content = output_file.read_text()

        # Should contain all tasks from sample workflow
        assert "process PREPARE_DATA {" in content
        assert "process ANALYZE_DATA {" in content
        assert "process GENERATE_REPORT {" in content

        # Should have proper workflow definition
        assert "workflow {" in content
