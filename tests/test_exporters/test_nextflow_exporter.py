"""Tests for the Nextflow exporter functionality."""

import pytest
from pathlib import Path
from wf2wf.core import Workflow, Task, ResourceSpec, EnvironmentSpec, Edge
from wf2wf.exporters.nextflow import from_workflow


class TestNextflowExporter:
    """Test the Nextflow exporter."""
    
    def test_export_simple_workflow(self, persistent_test_output):
        """Test exporting a simple linear workflow."""
        # Create a simple workflow
        wf = Workflow(name="simple_nextflow")
        
        task1 = Task(
            id="prepare_data",
            command="python prepare.py input.txt output.txt",
            inputs=["input.txt"],
            outputs=["output.txt"],
            resources=ResourceSpec(cpu=2, mem_mb=4096)
        )
        
        task2 = Task(
            id="analyze_data", 
            command="Rscript analyze.R output.txt results.txt",
            inputs=["output.txt"],
            outputs=["results.txt"],
            resources=ResourceSpec(cpu=4, mem_mb=8192),
            environment=EnvironmentSpec(container="rocker/r-ver:4.2.0")
        )
        
        wf.add_task(task1)
        wf.add_task(task2)
        wf.add_edge("prepare_data", "analyze_data")
        
        # Export to Nextflow
        output_file = persistent_test_output / "simple_workflow.nf"
        from_workflow(wf, output_file, verbose=True, modular=False)
        
        # Check that file was created
        assert output_file.exists()
        
        # Read and verify content
        content = output_file.read_text()
        
        # Check header
        assert "#!/usr/bin/env nextflow" in content
        assert "simple_nextflow - Generated by wf2wf" in content
        assert "nextflow.enable.dsl=2" in content
        
        # Check process definitions
        assert "process PREPARE_DATA {" in content
        assert "process ANALYZE_DATA {" in content
        
        # Check resource specifications
        assert "cpus 2" in content
        assert "memory '4.0.GB'" in content
        assert "cpus 4" in content
        assert "memory '8.0.GB'" in content
        
        # Check container
        assert "container 'rocker/r-ver:4.2.0'" in content
        
        # Check workflow definition
        assert "workflow {" in content
        assert "PREPARE_DATA(" in content
        assert "ANALYZE_DATA(PREPARE_DATA.out)" in content
        
        # Check config file was created
        config_file = persistent_test_output / "nextflow.config"
        assert config_file.exists()
    
    def test_export_modular_workflow(self, persistent_test_output):
        """Test exporting workflow with separate module files."""
        wf = Workflow(name="modular_workflow")
        
        task1 = Task(
            id="process_one",
            command="echo 'Process 1'",
            outputs=["file1.txt"],
            resources=ResourceSpec(cpu=1, mem_mb=2048),
            environment=EnvironmentSpec(container="ubuntu:20.04")
        )
        
        task2 = Task(
            id="process_two",
            command="echo 'Process 2'",
            inputs=["file1.txt"],
            outputs=["file2.txt"],
            resources=ResourceSpec(cpu=2, mem_mb=4096)
        )
        
        wf.add_task(task1)
        wf.add_task(task2)
        wf.add_edge("process_one", "process_two")
        
        # Export with modular=True (default)
        output_file = persistent_test_output / "modular_main.nf"
        from_workflow(wf, output_file, verbose=True)
        
        # Check main file
        main_content = output_file.read_text()
        assert "include { PROCESS_ONE } from './modules/process_one'" in main_content
        assert "include { PROCESS_TWO } from './modules/process_two'" in main_content
        
        # Check module files were created
        modules_dir = persistent_test_output / "modules"
        assert modules_dir.exists()
        
        module1_file = modules_dir / "process_one.nf"
        module2_file = modules_dir / "process_two.nf"
        assert module1_file.exists()
        assert module2_file.exists()
        
        # Check module content
        module1_content = module1_file.read_text()
        assert "process PROCESS_ONE {" in module1_content
        assert "container 'ubuntu:20.04'" in module1_content
        assert "echo 'Process 1'" in module1_content
        
        module2_content = module2_file.read_text()
        assert "process PROCESS_TWO {" in module2_content
        assert "cpus 2" in module2_content
    
    def test_export_with_config(self, persistent_test_output):
        """Test exporting workflow with configuration."""
        wf = Workflow(
            name="config_workflow",
            config={
                "input_data": "data/input.txt",
                "output_dir": "results",
                "threads": 8,
                "memory": "16GB",
                "debug": True
            }
        )
        
        task = Task(
            id="analyze",
            command="python analyze.py --input ${params.input_data} --threads ${params.threads}",
            resources=ResourceSpec(cpu=4, mem_mb=8192)
        )
        wf.add_task(task)
        
        # Export with separate config file
        output_file = persistent_test_output / "config_main.nf"
        config_file = persistent_test_output / "custom.config"
        
        from_workflow(wf, output_file, config_file=config_file)
        
        # Check config file content
        config_content = config_file.read_text()
        assert 'input_data = "data/input.txt"' in config_content
        assert 'output_dir = "results"' in config_content
        assert 'threads = 8' in config_content
        assert 'memory = "16GB"' in config_content
        assert 'debug = true' in config_content
    
    def test_export_with_containers_and_conda(self, persistent_test_output):
        """Test exporting workflow with container and conda specifications."""
        wf = Workflow(name="container_workflow")
        
        # Task with Docker container
        docker_task = Task(
            id="docker_task",
            command="python script.py",
            environment=EnvironmentSpec(container="docker://python:3.9-slim")
        )
        
        # Task with direct container reference
        container_task = Task(
            id="container_task",
            command="R script.R",
            environment=EnvironmentSpec(container="bioconductor/release_core2")
        )
        
        # Task with conda environment
        conda_task = Task(
            id="conda_task",
            command="snakemake --cores 1",
            environment=EnvironmentSpec(conda="envs/snakemake.yml")
        )
        
        wf.add_task(docker_task)
        wf.add_task(container_task)
        wf.add_task(conda_task)
        
        output_file = persistent_test_output / "container_workflow.nf"
        from_workflow(wf, output_file, modular=False)
        
        content = output_file.read_text()
        
        # Check container specifications (docker:// prefix should be removed)
        assert "container 'python:3.9-slim'" in content
        assert "container 'bioconductor/release_core2'" in content
        
        # Check conda specification
        assert "conda 'envs/snakemake.yml'" in content
    
    def test_export_with_comprehensive_resources(self, persistent_test_output):
        """Test exporting workflow with comprehensive resource specifications."""
        wf = Workflow(name="resource_workflow")
        
        task = Task(
            id="resource_intensive_task",
            command="python compute.py",
            resources=ResourceSpec(
                cpu=16,
                mem_mb=32768,  # 32GB
                disk_mb=102400,  # 100GB
                gpu=2,
                time_s=7200,  # 2 hours
                threads=8
            )
        )
        wf.add_task(task)
        
        output_file = persistent_test_output / "resource_workflow.nf"
        from_workflow(wf, output_file, modular=False)
        
        content = output_file.read_text()
        
        # Check resource conversion
        assert "cpus 16" in content
        assert "memory '32.0.GB'" in content
        assert "disk '100.0.GB'" in content
        assert "accelerator 2" in content
        assert "time '2.0h'" in content
    
    def test_export_with_retry_and_error_handling(self, persistent_test_output):
        """Test exporting workflow with retry and error handling."""
        wf = Workflow(name="retry_workflow")
        
        task1 = Task(
            id="reliable_task",
            command="python reliable.py",
            retry=0
        )
        
        task2 = Task(
            id="flaky_task", 
            command="python flaky.py",
            retry=3
        )
        
        wf.add_task(task1)
        wf.add_task(task2)
        
        output_file = persistent_test_output / "retry_workflow.nf"
        from_workflow(wf, output_file, modular=False)
        
        content = output_file.read_text()
        
        # Check that only the flaky task has retry settings
        assert content.count("errorStrategy 'retry'") == 1
        assert "maxRetries 3" in content
        
        # Check that reliable task doesn't have retry settings
        reliable_section = content[content.find("process RELIABLE_TASK"):content.find("process FLAKY_TASK")]
        assert "errorStrategy" not in reliable_section
        assert "maxRetries" not in reliable_section
    
    def test_export_with_scripts(self, persistent_test_output):
        """Test exporting workflow with script references."""
        wf = Workflow(name="script_workflow")
        
        task_with_script = Task(
            id="script_task",
            script="scripts/analysis.py",
            command="python scripts/analysis.py --input data.txt",
            inputs=["data.txt"],
            outputs=["results.txt"]
        )
        
        task_with_command = Task(
            id="command_task",
            command="echo 'Hello World' > hello.txt",
            outputs=["hello.txt"]
        )
        
        wf.add_task(task_with_script)
        wf.add_task(task_with_command)
        
        output_file = persistent_test_output / "script_workflow.nf"
        from_workflow(wf, output_file, modular=False)
        
        content = output_file.read_text()
        
        # Check script command
        assert "python scripts/analysis.py --input data.txt" in content
        
        # Check inline command
        assert "echo 'Hello World' > hello.txt" in content
    
    def test_export_complex_dependencies(self, persistent_test_output):
        """Test exporting workflow with complex dependency patterns."""
        wf = Workflow(name="complex_workflow")
        
        # Create a diamond dependency pattern
        task_a = Task(id="task_a", command="echo A", outputs=["a.txt"])
        task_b = Task(id="task_b", command="echo B", inputs=["a.txt"], outputs=["b.txt"])
        task_c = Task(id="task_c", command="echo C", inputs=["a.txt"], outputs=["c.txt"])
        task_d = Task(id="task_d", command="echo D", inputs=["b.txt", "c.txt"], outputs=["d.txt"])
        
        wf.add_task(task_a)
        wf.add_task(task_b)
        wf.add_task(task_c)
        wf.add_task(task_d)
        
        wf.add_edge("task_a", "task_b")
        wf.add_edge("task_a", "task_c")
        wf.add_edge("task_b", "task_d")
        wf.add_edge("task_c", "task_d")
        
        output_file = persistent_test_output / "complex_workflow.nf"
        from_workflow(wf, output_file, modular=False)
        
        content = output_file.read_text()
        
        # Check workflow execution order and dependencies
        assert "TASK_A(task_a_ch)" in content  # Input task gets a channel
        assert "TASK_B(TASK_A.out)" in content
        assert "TASK_C(TASK_A.out)" in content
        assert "TASK_D(TASK_B.out, TASK_C.out)" in content or "TASK_D(TASK_C.out, TASK_B.out)" in content
    
    def test_name_sanitization(self, persistent_test_output):
        """Test that invalid process names are sanitized."""
        wf = Workflow(name="sanitize_workflow")
        
        # Tasks with problematic names
        task1 = Task(id="task-with-dashes", command="echo test1")
        task2 = Task(id="task.with.dots", command="echo test2")
        task3 = Task(id="123_numeric_start", command="echo test3")
        task4 = Task(id="task with spaces", command="echo test4")
        
        wf.add_task(task1)
        wf.add_task(task2)
        wf.add_task(task3)
        wf.add_task(task4)
        
        output_file = persistent_test_output / "sanitize_workflow.nf"
        from_workflow(wf, output_file, modular=False)
        
        content = output_file.read_text()
        
        # Check sanitized process names
        assert "process TASK_WITH_DASHES {" in content
        assert "process TASK_WITH_DOTS {" in content
        assert "process PROCESS_123_NUMERIC_START {" in content
        assert "process TASK_WITH_SPACES {" in content
    
    def test_memory_unit_conversion(self, persistent_test_output):
        """Test proper conversion of memory units."""
        wf = Workflow(name="memory_test")
        
        # Test various memory sizes
        task_mb = Task(id="task_mb", resources=ResourceSpec(mem_mb=512))
        task_gb = Task(id="task_gb", resources=ResourceSpec(mem_mb=4096))  # 4GB
        task_large = Task(id="task_large", resources=ResourceSpec(mem_mb=65536))  # 64GB
        
        wf.add_task(task_mb)
        wf.add_task(task_gb)
        wf.add_task(task_large)
        
        output_file = persistent_test_output / "memory_test.nf"
        from_workflow(wf, output_file, modular=False)
        
        content = output_file.read_text()
        
        # Check memory conversions
        assert "memory '512.MB'" in content  # Small memory stays in MB
        assert "memory '4.0.GB'" in content  # >= 1GB converted to GB
        assert "memory '64.0.GB'" in content  # Large memory in GB
    
    def test_time_unit_conversion(self, persistent_test_output):
        """Test proper conversion of time units."""
        wf = Workflow(name="time_test")
        
        # Test various time durations
        task_minutes = Task(id="task_min", resources=ResourceSpec(time_s=1800))  # 30 minutes
        task_hours = Task(id="task_hour", resources=ResourceSpec(time_s=7200))  # 2 hours
        task_long = Task(id="task_long", resources=ResourceSpec(time_s=28800))  # 8 hours
        
        wf.add_task(task_minutes)
        wf.add_task(task_hours)
        wf.add_task(task_long)
        
        output_file = persistent_test_output / "time_test.nf"
        from_workflow(wf, output_file, modular=False)
        
        content = output_file.read_text()
        
        # Check time conversions
        assert "time '30m'" in content  # < 1 hour in minutes
        assert "time '2.0h'" in content  # >= 1 hour in hours
        assert "time '8.0h'" in content  # Long duration in hours
    
    def test_complex_workflow_from_json(self, sample_workflow_json, persistent_test_output):
        """Test exporting the sample JSON workflow to Nextflow."""
        from wf2wf.core import Workflow
        
        # Load the sample workflow
        wf = Workflow.load_json(sample_workflow_json)
        
        # Export to Nextflow
        output_file = persistent_test_output / "sample_workflow.nf"
        from_workflow(wf, output_file, verbose=True, modular=False)
        
        assert output_file.exists()
        
        content = output_file.read_text()
        
        # Should contain all tasks from sample workflow
        assert "process PREPARE_DATA {" in content
        assert "process ANALYZE {" in content
        assert "process GENERATE_REPORT {" in content
        
        # Should have proper workflow definition
        assert "workflow {" in content 